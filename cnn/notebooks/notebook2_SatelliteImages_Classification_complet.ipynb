{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##############################################################################\n",
    "##############################################################################\n",
    "### **Atelier \"Comprendre et développer des CNN pour le traitement d'images\"**\n",
    "##############################################################################\n",
    "##############################################################################\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "#### **Partie 2 : Développement d'un CNN de classification d'images satellitaires from scratch** \n",
    "\n",
    "\n",
    "\n",
    "Dans cette deuxième partie, nous allons développer un modèle CNN \"from scratch\", afin de classifier correctement des images satellitaires de 4 types : désert, foret, océan et nuageux\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "############################################################\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "#### **Import des librairies utiles pour l'atelier**\n",
    "\n",
    "L'atelier se base sur **Pytorch**, un framework qui a énormément gagné en popularité ces dernières années. \n",
    "\n",
    "**TorchVision** est lui un package spécialement tourné vers la Computer Vision, et permet l'accès facile à des datasets publics, de créer aisement ses propres datasets. Il contient aussi nativement les principaux modèlesde fondation Open-Source, comme nous le verrons plus tard.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms, models\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Vérification de la présence de capacité de calcul GPU sur l'ordinateur**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "#### **Etape 1 : chargement du Dataset et transformation des données pour qu'elles soient utilisables par un modèle CNN**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "Nous allons maintenant transformer les images pour qu'elles aient : \n",
    "- La taille requise pour être entrée dans le modèle : 128 pixels par 128 pixels\n",
    "- Un format qui soit un tensor Pytorch, afin d'être intégré au Pipeline Pytorch\n",
    "\n",
    "\n",
    "Nous allons utiliser l'utilitaire **Transforms de TorchVision** qui va automatiser le pré-traitement des éléments du dataset\n",
    "<br>\n",
    "\n",
    "#### **Chargement du Dataset avec l'utilitaire datasets de TorchVision**\n",
    "\n",
    "TorchVision permet de manipuler de manière optimale les datasets, en leur appliquant des pipelines de transformation en optimisant le temps de calcul\n",
    "\n",
    "La seule obligation est de fournir un répertoire où les images doivent être mises dans des dossiers contenant le nom des labels. Ces noms de dossier deviendront automatiquement les classes d'apprentissage\n",
    "\n",
    "<br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "data_dir = \"C:/Code_ML/Code/Atelier CNN/data/\"\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),  # Redimensionner les images\n",
    "    transforms.ToTensor(),          # Convertir en tenseur\n",
    "    \n",
    "])\n",
    "full_dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n",
    "print(f\"Total Images: {len(full_dataset)}\")\n",
    "print(f\"Classes: {full_dataset.classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Nous n'avons pas normalisé nos données, nous allons donc calculer les propriétés du dataset et appliquer ces valeurs à l'opération de normalisation\n",
    "\n",
    "<br>\n",
    "\n",
    "Ce qu'il faut savoir :\n",
    "\n",
    "les images dans PyTorch sont stockées sous la forme d’un tenseur 4D : \n",
    "    images.shape = (B,C,H,W)\n",
    "\n",
    "où :\n",
    "- B : batch size\n",
    "- C : nombre de canaux\n",
    "- H : hauteur de l'image\n",
    "- W : largeur de l'image\n",
    "\n",
    "<br>\n",
    "\n",
    "Si l'on veut par exemple calculer la moyenne sur une image, elle se mesure par canal, R, G et B par exemple. Cette moyenne est donc un tenseur de taille C, elle est calculée par l'opération suivante : images.mean([0,2,3]), soit la moyenne sur tout le batch, calculé sur tous les pixels\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "full_dataset_loader = DataLoader(full_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "# Fonction pour calculer la moyenne et l'écart-type\n",
    "def compute_mean_std(full_dataset_loader):\n",
    "    mean = torch.zeros(3)\n",
    "    std = torch.zeros(3)\n",
    "    total_images = 0\n",
    "\n",
    "    for images, _ in full_dataset_loader:\n",
    "        batch_size, channels, height, width = images.shape\n",
    "        total_images += batch_size\n",
    "        \n",
    "         \n",
    "        mean +=  images.mean([0, 2, 3]) * batch_size\n",
    "        std += images.std([0, 2, 3]) * batch_size\n",
    "\n",
    "    mean /= total_images\n",
    "    std /= total_images\n",
    "\n",
    "    return mean, std\n",
    "\n",
    "# Calcul des statistiques\n",
    "mean, std = compute_mean_std(full_dataset_loader)\n",
    "\n",
    "print(f\"Moyenne des canaux RGB: {mean}\")\n",
    "print(f\"Écart-type des canaux RGB: {std}\")\n",
    "\n",
    "\n",
    "\n",
    " # Nouvelle transformation avec normalisation\n",
    "full_dataset.transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),  # Redimensionner les images\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean.tolist(), std=std.tolist())\n",
    " ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "#### **Split du dataset**\n",
    "\n",
    "Selon les bonnes pratiques du ML, nous allons spliter notre dataset en jeu de Train, Validation et Test. Nous allons ensuite les convertir en Loaders, qui permettront une manipulation optimale ultérieurement.\n",
    "\n",
    "\n",
    "Le paramètre \"Batch size\" détermine la taille des batchs servant à itérer pour chaque mise à jour des poids.\n",
    "\n",
    "\n",
    "On pourrait passer 1 image à la fois ou à l'inverse, tout le dataset, mais ce n'est pas optimal. Passer des batchs de taille adaptée permet de gagner en efficacité d'entrainemenet et en stabilité des gradients\n",
    "\n",
    "<br>\n",
    "\n",
    "Les proportions retenues entre les jeux de train, validation et test peuvent varier, mais pour des datasets de taille moyenne, on choisit un ratio de type [70%,15%,15%]\n",
    "\n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.7 * len(full_dataset))\n",
    "val_size = int(0.15 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(full_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"Train: {len(train_dataset)}, Validation: {len(val_dataset)}, Test: {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Avant de créer le modèle, visualisations quelques échantillons aléatoires du dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = full_dataset.classes\n",
    "def denormalize(image, mean, std):\n",
    "    mean = torch.tensor(mean).view(3, 1, 1)  # Adapter la forme pour le broadcasting\n",
    "    std = torch.tensor(std).view(3, 1, 1)\n",
    "    return image * std + mean  # Correcte inversion de la normalisation\n",
    "def show_images(loader):\n",
    "    dataiter = iter(loader)\n",
    "    images, labels = next(dataiter)\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(15, 5))\n",
    "    for i in range(5):\n",
    "        img = denormalize(images[i], mean, std)  # Dénormaliser l'image\n",
    "        img = img.permute(1, 2, 0).clamp(0, 1)  # Convertir en format HWC et s'assurer des valeurs valides [0,1]\n",
    "\n",
    "        axes[i].imshow(img.numpy())  \n",
    "        axes[i].set_title(classes[labels[i]])\n",
    "        axes[i].axis('off')\n",
    "    plt.show()\n",
    "\n",
    "print(\"Sample images from training data:\")\n",
    "show_images(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "#### **Etape 2 : construction de l'architecture du CNN**\n",
    "\n",
    "\n",
    "Nous allons maintenant constituer l'architecture de notre CNN.\n",
    "\n",
    "\n",
    "Pour cela, Pytorch propose la définition d'un classe CNN dans lequel nous aurons définir 2 parties :\n",
    "<br>\n",
    "\n",
    "------------------------------------------------------------------------------------------------------------------------\n",
    "------------------------------------------------------------------------------------------------------------------------\n",
    "- **self.conv_layers** : dans laquelle nous allons ajouter de manière séquentielle les différentes strates de la partie convolutive du modèle avec leurs paramètres\n",
    "    -   Exemple : **nn.Conv2d(3, 16, kernel_size=3, stride=2, padding=1)**\n",
    "\n",
    "                - couche de convolution avec 16 matrices de convolution de taille 3X3X3, l'image ayant 3 canaux. La couche aura donc en sortie 32 cartes de features issus des 32 matrices.\n",
    "\n",
    "                - stride = 2: le noyau de convolution se déplace 2 pixel par 2 pixel, ce qui créé une diminution de la taille de l'image\n",
    "\n",
    "                - padding = 1 : On rajoute 1 pixel en bord d'image pour que le passage du filtre ne réduise pas la taille de la sortie \n",
    "\n",
    "------------------------------------------------------------------------------------------------------------------------\n",
    "------------------------------------------------------------------------------------------------------------------------          \n",
    "- **self.fc_layers** : dans laquelle nous allons ajouter de manière séquentielle les différentes strates de la partie \"fully connected\" du modèle avec leurs paramètres. Généralement, on va aplatir les données en sortie de la partie conventionneles pour les transformer en vecteur unidimensionnel (\"flatten\") et ainsi pouvoir ajouter une couche de neurones totalement connectés, sur 1 ou plusieurs couches, jusqu'à la couche finale qui contient autant de neurons que le nombre de classes à prédire (4 dans notre cas)\n",
    "\n",
    "<br>\n",
    "\n",
    "La fonction forward est standard et indique l'ordre de propagation des données dans le modèle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            \n",
    "         \n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1) ,\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2) ,\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1) ,\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2) ,\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1) ,\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), \n",
    "        )\n",
    "\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 16 * 16, 256),  \n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "            \n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instantiation du modèle**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN(num_classes=len(classes))\n",
    "model = model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### **Etape 3 : entrainement**\n",
    "\n",
    "\n",
    "Nous allons maintenant procéder à l'entrainement du modèle.\n",
    "\n",
    "\n",
    "Nous commencons par définir la métrique liée à la fonction de cout (ici l'entropie croisée)\n",
    "\n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Nous définissons ensuite l'**optimizer**, le grand classique étant Adam.\n",
    "\n",
    "Regardons le paramètre lr ou **learning rate**:\n",
    "Ce paramètre détermine l'intensité de mise à jour des poids du modèle. Ceci signifie qu'un lr trop grand engendre de fortes variations et donc une forte instabilité, alors qu'un lr trop faible rendrait très lent l'apprentissage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Nous allons maintenant entrainer le modèle sur un certain nombre d'époques, ici définis à 5.\n",
    "<br>\n",
    "\n",
    "Lors de chaque époque, le modèle va s'entrainer sur tous les éléments du dataset de train, puis sera évalué sur le dataset de validation.\n",
    "\n",
    "l'objet Model a 2 modes, le \"training mode\" enclenché par l'appel model.train() et le \"validation mode, enclenché par l'appel model.eval().\n",
    "\n",
    "<br>\n",
    "\n",
    "2 élements clefs à comprendre :\n",
    "\n",
    "- **lors de la première phase** :\n",
    "    1. la \"forward pass\" se passe lorsque l'input est passée dans le modèle pour générer des outputs  :outputs = model(inputs)\n",
    "    2. l'erreur est calculée : loss = criterion(outputs, labels)\n",
    "    3. la retropropagation d'erreur suit : loss.backward()\n",
    "    4. les poids du modèles sont mis à jour : optimizer.step() \n",
    "- **lors de la deuxième phase**, l'erreur sur le jeu de validation est calculé. Cette valeur sert exclusivement à observer du surapprentissage\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracy = 100 * correct / total\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Affichage des valeurs de loss et d'accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot validation losses\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, num_epochs + 1), val_losses, marker='o', color='b', label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Validation Loss over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### **Etape 4 : Evaluation du modèle sur les données de Test, données extérieures à l'apprentissage**\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_loss /= len(test_loader)\n",
    "test_accuracy = 100 * correct / total\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "#### **Etape 5 : à vous de jouer !**\n",
    "\n",
    "Prenez le temps d'étudier l'effet de différents paramètres :\n",
    "- l'ajout d'une data augmentation\n",
    "- la variation du batch size\n",
    "- la variation du learning rate\n",
    "- l'ajout de couches de convolution\n",
    "\n",
    "<br>\n",
    "\n",
    "**Question bonus** : pouvez-vous retrouver, à partir des caractéristiques du CNN initial et de sa première couche fully connected (nn.Linear(128 * 16 * 16, 256)), la nécessité d'avoir des images de format 128x128x3 en entrée?\n",
    "\n",
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
